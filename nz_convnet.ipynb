{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g5lGLV8QePSF"
   },
   "source": [
    "# Part 0 - Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "66-15hZ5n4Zc"
   },
   "outputs": [],
   "source": [
    "## On Google Colaboratory only, install google drive stuff\n",
    "'''\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9g04cjQMn56O"
   },
   "outputs": [],
   "source": [
    "## On Google Colaboratory only, link google drive\n",
    "'''\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4FrjWr__eixk"
   },
   "outputs": [],
   "source": [
    "## On Google Colaboratory only, install necessary dependencies\n",
    "#!pip install keras tensorflow-gpu && apt update && apt install -y python3-gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qI5gAk9lePSI"
   },
   "outputs": [],
   "source": [
    "# Set number of GPUs\n",
    "num_gpus = 4   #defaults to 1 if one-GPU or one-CPU. If 4 GPUs, set to 4.\n",
    "\n",
    "# Set height (y-axis length) and width (x-axis length) to train model on\n",
    "img_height, img_width = (256,256)  #Default to (256,266), use (None,None) if you do not want to resize imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fRRjiHfCePST"
   },
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "import os\n",
    "import datetime\n",
    "import glob\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io                                     #Used for imshow function\n",
    "import skimage.transform                              #Used for resize function\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Conv2DTranspose\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda, AlphaDropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import load_model, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import plot_model, multi_gpu_model\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import ogr\n",
    "import gdal\n",
    "\n",
    "print('Python       :', sys.version.split('\\n')[0])\n",
    "print('Numpy        :', np.__version__)\n",
    "print('Skimage      :', skimage.__version__)\n",
    "print('Scikit-learn :', sklearn.__version__)\n",
    "print('Keras        :', keras.__version__)\n",
    "print('Tensorflow   :', tf.__version__)\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "q_C6ByviePSo"
   },
   "outputs": [],
   "source": [
    "# Set seed values\n",
    "seed = 42\n",
    "random.seed = seed\n",
    "np.random.seed(seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2rF2A1OMePSs"
   },
   "outputs": [],
   "source": [
    "# Have a look at our data folder\n",
    "topDir = os.getcwd()+\"/data\"  #default top directory\n",
    "#topDir = \"/content/drive/Colab Notebooks/data\"  #default top directory on Google Colab\n",
    "os.chdir(topDir)\n",
    "print(os.listdir())\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9NMEiG8BePSx"
   },
   "source": [
    "# Part 1 - Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ovf2b4LgePSz"
   },
   "outputs": [],
   "source": [
    "# https://pcjericks.github.io/py-gdalogr-cookbook/raster_layers.html\n",
    "# https://gis.stackexchange.com/questions/16837/how-can-i-turn-a-shapefile-into-a-mask-and-calculate-the-mean\n",
    "def vectorPoly_to_rasterMask(vector, raster, suffix='_mask', output=None, show=False):\n",
    "    \"\"\"\n",
    "    Function to turn a vector Polygon (ogr) into a raster binary mask (gdal) of 1 for present, 0 for absent\n",
    "    \n",
    "    Outputs a raster geotiff with extents according to the input raster.\n",
    "    \"\"\"\n",
    "    if output==None:\n",
    "        output = raster.split('.')[0]+suffix+\".\"+raster.split('.')[-1]\n",
    "    \n",
    "    \n",
    "    ## Open raster data source\n",
    "    assert(os.path.exists(raster))\n",
    "    raster_ds = gdal.Open(raster)\n",
    "    raster_prj = raster_ds.GetProjection()\n",
    "    \n",
    "    ulx, px, rx, uly, ry, py = raster_ds.GetGeoTransform()  #upper left X, pixel resolution X, rotation X, upper left Y, pixel resolution Y, rotation Y\n",
    "    px, py = round(px,2), round(py,2)  #round pixel size to two decimal places\n",
    "    width, height = raster_ds.RasterXSize, raster_ds.RasterYSize  #number of x columns and number of y rows\n",
    "    #print(ulx, px, rx, uly, ry, py), (width, height)\n",
    "    '''\n",
    "\n",
    "      ul-------ur\n",
    "    ^  |       |\n",
    "    |  |  geo  |    y increases going up, x increases going right\n",
    "    y  |       |\n",
    "      ll-------lr\n",
    "          x-->\n",
    "\n",
    "    '''\n",
    "    assert(rx==0 and ry==0)   #assuming zero rotation!!\n",
    "    llx = ulx\n",
    "    lly = uly + (height * py)\n",
    "    urx = ulx + (width * px)\n",
    "    ury = uly\n",
    "    bbox = (llx, lly, urx, ury)  #minx, miny, maxx, maxy\n",
    "    print(\"min_xy:({0},{1}), max_xy:({2},{3})\".format(*bbox))\n",
    "    \n",
    "    \n",
    "    ## Open vector data source\n",
    "    assert(os.path.exists(vector))\n",
    "    vector_ds = ogr.Open(vector)\n",
    "    vector_lyr = vector_ds.GetLayer()\n",
    "    vector_prj = vector_lyr.GetSpatialRef()\n",
    "    vector_ext = vector_lyr.GetExtent()   #x_min, x_max, y_min, y_max\n",
    "    #print(vector_ext)\n",
    "    \n",
    "    \n",
    "    ## Create the raster mask according to input raster dimensions\n",
    "    #print((urx - llx) / px, (ury - lly) / abs(py))\n",
    "    x_res = int(round((urx - llx) / px, 0))\n",
    "    y_res = int(round((ury - lly) / abs(py), 0))  #turn negative pixel size y to absolute value\n",
    "    #print(x_res, y_res)\n",
    "    target_ds = gdal.GetDriverByName('GTiff').Create(output, x_res, y_res, 1, gdal.GDT_Byte)\n",
    "    target_ds.SetGeoTransform((llx, px, rx, uly, ry, py))\n",
    "    target_ds.SetProjection(raster_prj)\n",
    "    band = target_ds.GetRasterBand(1)\n",
    "    band.SetNoDataValue(np.nan)\n",
    "    \n",
    "    ## Rasterize\n",
    "    err = gdal.RasterizeLayer(target_ds, [1], vector_lyr, None, None, [1], ['ALL_TOUCHED=TRUE'])\n",
    "    target_ds.FlushCache()\n",
    "    \n",
    "    \n",
    "    ## Create output arrays\n",
    "    img_ary = np.dstack([raster_ds.GetRasterBand(i).ReadAsArray() for i in range(1,4)])\n",
    "    msk_ary = target_ds.GetRasterBand(1).ReadAsArray()\n",
    "   \n",
    "    \n",
    "    ## Visualize the raster with its output mask\n",
    "    if show==True:\n",
    "        #f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "        skimage.io.imshow(img_ary)\n",
    "        plt.show()\n",
    "        skimage.io.imshow(msk_ary)\n",
    "        plt.show()\n",
    "        #skimage.io.imsave(output, mask_ary)\n",
    "    \n",
    "    ## Final checks and turn mask into boolean array\n",
    "    #print(img_ary.shape[:2], msk_ary.shape)\n",
    "    assert(img_ary.shape[:2]==msk_ary.shape)   #check that shape of image and mask are the same\n",
    "    msk_ary = skimage.transform.resize(msk_ary, output_shape=msk_ary.shape+(1,), mode='constant', preserve_range=True)  #need to add an extra dimension so mask.shape = (img_height, img_width, 1)\n",
    "    msk_ary = msk_ary.astype(bool)  #convert to binary mask of either 0 or 1\n",
    "    \n",
    "    return img_ary, msk_ary\n",
    "\n",
    "def ary_to_tiles(ary, shape=(256,256), exclude_empty=False):\n",
    "    \"\"\"\n",
    "    Function to turn a big 2D numpy array (image) and tile it into a set number of shapes\n",
    "    \n",
    "    Outputs a stacked numpy array suitable for input into a Convolutional Neural Network\n",
    "    \"\"\"\n",
    "    assert(isinstance(ary, np.ndarray))\n",
    "    assert(isinstance(shape, tuple))\n",
    "    \n",
    "    ary_height, ary_width = shape\n",
    "    ary_list = []\n",
    "    \n",
    "    for x_step in range(0, ary.shape[0], ary_width):\n",
    "        for y_step in range(0, ary.shape[1], ary_height):\n",
    "            x0, x1 = x_step, x_step+img_width\n",
    "            y0, y1 = y_step, y_step+img_height\n",
    "            \n",
    "            crop_ary = ary[y0:y1, x0:x1]\n",
    "            try:\n",
    "                assert(crop_ary.shape == (ary_height, ary_width, ary.shape[2]))  #do not include images not matching the intended size\n",
    "                if exclude_empty == True:\n",
    "                    assert(np.any(crop_ary) == True)          #do not include images without a mask\n",
    "            except AssertionError:\n",
    "                #print(crop_X_ary.shape)\n",
    "                continue\n",
    "            ary_list.append(crop_ary)\n",
    "    \n",
    "    return np.stack(ary_list)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "B9zORjtGePS4"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('X_data.npy') or not os.path.exists('Y_data.npy'):\n",
    "    for i, raster in enumerate(glob.glob(topDir+\"/raster/*[!_mask].tif\")):\n",
    "        #img_ary, msk_ary = vectorPoly_to_rasterMask(vector='nz-building-outlines-pilot.shp', raster='RGB_BX24_5K_0401.tif', output='tmp_mask.tif')\n",
    "        img_ary_tmp, msk_ary_tmp = vectorPoly_to_rasterMask(vector='vector/nz-building-outlines-pilot.shp', raster=raster)\n",
    "        X_data_tmp = ary_to_tiles(img_ary_tmp, shape=(img_height, img_width))\n",
    "        Y_data_tmp = ary_to_tiles(msk_ary_tmp, shape=(img_height, img_width))\n",
    "        print(i, X_data_tmp.shape)\n",
    "        try:\n",
    "            X_data = np.concatenate([X_data, X_data_tmp], axis=0)\n",
    "            Y_data = np.concatenate([Y_data, Y_data_tmp], axis=0)\n",
    "        except NameError:\n",
    "            X_data = X_data_tmp\n",
    "            Y_data = Y_data_tmp\n",
    "        print(i, 'appended', X_data.shape)\n",
    "    # Save full datasets first\n",
    "    np.save('X_data_full.npy', X_data)\n",
    "    np.save('Y_data_full.npy', Y_data)\n",
    "    # Clean up empty masks\n",
    "    zero_mask = (Y_data == 0).all(axis=(1,2,3))  #get a zero_mask of indexes where there are no building masks\n",
    "    Y_data = Y_data[~zero_mask]    #apply zero_mask to Y_train (masks)\n",
    "    X_data = X_data[~zero_mask]    #apply zero_mask to X_train (images)\n",
    "    # Save array to disk\n",
    "    np.save('X_data.npy', X_data)\n",
    "    np.save('Y_data.npy', Y_data)\n",
    "elif os.path.exists('X_data.npy') and os.path.exists('Y_data.npy'):\n",
    "    X_data = np.load('X_data.npy')\n",
    "    Y_data = np.load('Y_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rUWqcTm6ePS9"
   },
   "outputs": [],
   "source": [
    "print(X_data.shape, X_data.dtype)\n",
    "print(Y_data.shape, Y_data.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JVZ7SdFePTJ"
   },
   "source": [
    "## Visualize masks on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TxXXzV4UePTO"
   },
   "outputs": [],
   "source": [
    "id = 128\n",
    "print(X_data[id].shape)\n",
    "skimage.io.imshow(X_data[id])\n",
    "plt.show()\n",
    "skimage.io.imshow(Y_data[id][:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_wbpZL4ePTX"
   },
   "source": [
    "# Part 2 - Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3xuKiVKtePTY"
   },
   "outputs": [],
   "source": [
    "# Custom IoU metric\n",
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.50, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score) \n",
    "        return K.mean(K.stack(prec), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QbJhtGqDePTd"
   },
   "outputs": [],
   "source": [
    "# Design our model architecture here\n",
    "def keras_model(img_width=256, img_height=256, tensorboard_images=False):\n",
    "    '''\n",
    "    Modified from https://keunwoochoi.wordpress.com/2017/10/11/u-net-on-keras-2-0/\n",
    "    Architecture inspired by https://blog.deepsense.ai/deep-learning-for-satellite-imagery-via-image-segmentation/\n",
    "    '''\n",
    "    #n_ch_exps = [4, 5, 6, 7, 8]   #the n-th deep channel's exponent i.e. 2**n 16,32,64,128,256\n",
    "    n_ch_exps = [6, 6, 6, 6, 6]\n",
    "    k_size = (3, 3)                     #size of filter kernel\n",
    "    k_init = 'lecun_normal'             #kernel initializer\n",
    "    activation = 'selu'\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        ch_axis = 1\n",
    "        input_shape = (3, img_width, img_height)\n",
    "    elif K.image_data_format() == 'channels_last':\n",
    "        ch_axis = 3\n",
    "        input_shape = (img_width, img_height, 3)\n",
    "\n",
    "    inp = Input(shape=input_shape)\n",
    "    if tensorboard_images == True:\n",
    "        tf.summary.image(name='input', tensor=inp)\n",
    "    encodeds = []\n",
    "\n",
    "    # encoder\n",
    "    enc = inp\n",
    "    print(n_ch_exps)\n",
    "    for l_idx, n_ch in enumerate(n_ch_exps):\n",
    "        with K.name_scope('Encode_block_'+str(l_idx)):\n",
    "            enc = Conv2D(filters=2**n_ch, kernel_size=k_size, activation=activation, padding='same', kernel_initializer=k_init)(enc)\n",
    "            enc = AlphaDropout(0.1*l_idx,)(enc)\n",
    "            enc = Conv2D(filters=2**n_ch, kernel_size=k_size, dilation_rate=(2,2), activation=activation, padding='same', kernel_initializer=k_init)(enc)\n",
    "            encodeds.append(enc)\n",
    "            #print(l_idx, enc)\n",
    "            if l_idx < len(n_ch_exps)-1:  #do not run max pooling on the last encoding/downsampling step\n",
    "                enc = MaxPooling2D(pool_size=(2,2))(enc)  #strides = pool_size if strides is not set\n",
    "                #enc = Conv2D(filters=2**n_ch, kernel_size=k_size, strides=(2,2), activation=activation, padding='same', kernel_initializer=k_init)(enc)\n",
    "            if tensorboard_images == True:\n",
    "                tf.summary.histogram(\"conv_encoder\", enc)\n",
    "            \n",
    "    # decoder\n",
    "    dec = enc\n",
    "    print(n_ch_exps[::-1][1:])\n",
    "    decoder_n_chs = n_ch_exps[::-1][1:]\n",
    "    for l_idx, n_ch in enumerate(decoder_n_chs):\n",
    "        with K.name_scope('Decode_block_'+str(l_idx)):\n",
    "            l_idx_rev = len(n_ch_exps) - l_idx - 1  #\n",
    "            dec = concatenate([dec, encodeds[l_idx_rev]], axis=ch_axis)\n",
    "            dec = Conv2D(filters=2**n_ch, kernel_size=k_size, dilation_rate=(2,2), activation=activation, padding='same', kernel_initializer=k_init)(dec)\n",
    "            dec = AlphaDropout(0.1*l_idx)(dec)\n",
    "            dec = Conv2D(filters=2**n_ch, kernel_size=k_size, activation=activation, padding='same', kernel_initializer=k_init)(dec)\n",
    "            dec = Conv2DTranspose(filters=2**n_ch, kernel_size=k_size, strides=(2,2), activation=activation, padding='same', kernel_initializer=k_init)(dec)\n",
    "\n",
    "    outp = Conv2DTranspose(filters=1, kernel_size=k_size, activation='sigmoid', padding='same', kernel_initializer='glorot_normal')(dec)\n",
    "    if tensorboard_images == True:\n",
    "        tf.summary.image(name='output', tensor=outp)\n",
    "    \n",
    "    model = Model(inputs=[inp], outputs=[outp])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5LqjzxygePTj",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set some model compile parameters\n",
    "optimizer = keras.optimizers.Adam()\n",
    "loss      = 'binary_crossentropy'\n",
    "metrics   = [mean_iou]\n",
    "\n",
    "# Compile our model\n",
    "model = keras_model(img_width=img_width, img_height=img_height, tensorboard_images=True)\n",
    "model.summary()\n",
    "\n",
    "# For more GPUs\n",
    "if num_gpus > 1:\n",
    "    model = multi_gpu_model(model, gpus=num_gpus)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCkN9qnRePTt"
   },
   "source": [
    "# Part 3 - Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "c1zn2EfsePTv"
   },
   "outputs": [],
   "source": [
    "# Runtime custom callbacks\n",
    "\n",
    "# Live loss plot from https://github.com/deepsense-ai/intel-ai-webinar-neural-networks/blob/master/live_loss_plot.py\n",
    "# Fixed code to enable non-flat loss plots on keras model.fit_generator()\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import Callback\n",
    "from IPython.display import clear_output\n",
    "#from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "def translate_metric(x):\n",
    "    translations = {'acc': \"Accuracy\", 'loss': \"Log-loss (cost function)\"}\n",
    "    if x in translations:\n",
    "        return translations[x]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "class PlotLosses(Callback):\n",
    "    def __init__(self, figsize=None):\n",
    "        super(PlotLosses, self).__init__()\n",
    "        self.figsize = figsize\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "\n",
    "        self.base_metrics = [metric for metric in self.params['metrics'] if not metric.startswith('val_')]\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs.copy())\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=self.figsize)\n",
    "        \n",
    "        for metric_id, metric in enumerate(self.base_metrics):\n",
    "            plt.subplot(1, len(self.base_metrics), metric_id + 1)\n",
    "            \n",
    "            plt.plot(range(1, len(self.logs) + 1),\n",
    "                     [log[metric] for log in self.logs],\n",
    "                     label=\"training\")\n",
    "            if self.params['do_validation']:\n",
    "                plt.plot(range(1, len(self.logs) + 1),\n",
    "                         [log['val_' + metric] for log in self.logs],\n",
    "                         label=\"validation\")\n",
    "            plt.title(translate_metric(metric))\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(loc='center left')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show();\n",
    "\n",
    "plot_losses = PlotLosses(figsize=(16, 4))\n",
    "\n",
    "# Tensorboard\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1, write_graph=True, write_images=True)\n",
    "\n",
    "# Model Checkpoints\n",
    "if not os.path.exists(topDir+\"/model\"):\n",
    "    os.makedirs(topDir+\"/model\")\n",
    "filepath=\"model/weights-{epoch:02d}-{val_mean_iou:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_mean_iou', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "\n",
    "# Bring all the callbacks together into a python list\n",
    "callbackList = [plot_losses, tensorboard, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0unalDgLePT0"
   },
   "outputs": [],
   "source": [
    "validation_split = 0.20\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data,\n",
    "                                                    Y_data,\n",
    "                                                    train_size=1-validation_split,\n",
    "                                                    test_size=validation_split,\n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bMU4RkHzePT3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Finally train the model!!\n",
    "batch_size = 32\n",
    "\n",
    "try:  #to load a checkpoint file if it exists\n",
    "    checkfile = sorted(glob.glob(topDir+\"/model/weights-*-*.hdf5\"))[-1]\n",
    "    model.load_weights(checkfile)\n",
    "    initial_epoch = int(re.search(r\"weights-(\\d*)-\", checkfile).group(1))\n",
    "    print(\"Model weights loaded, resuming from epoch {0}\".format(initial_epoch))\n",
    "except IndexError:\n",
    "    initial_epoch = 0\n",
    "    pass\n",
    "\n",
    "model.fit(x=X_train, y=Y_train, verbose=1, validation_split=0.25, batch_size=batch_size, epochs=50, callbacks=callbackList, initial_epoch=initial_epoch)\n",
    "#model.fit(x=X_train, y=Y_train, verbose=1, validation_data=(X_test, Y_test), batch_size=batch_size, epochs=100, callbacks=callbackList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-I8-GyIeePT-"
   },
   "outputs": [],
   "source": [
    "# Save the model weights to a hdf5 file\n",
    "if num_gpus > 1:\n",
    "    #Refer to https://stackoverflow.com/questions/41342098/keras-load-checkpoint-weights-hdf5-generated-by-multiple-gpus\n",
    "    #model.summary()\n",
    "    model_out = model.layers[-2]  #get second last layer in multi_gpu_model i.e. model.get_layer('model_1')\n",
    "else:\n",
    "    model_out = model\n",
    "model_out.save_weights(filepath=topDir+\"/model/model-weights.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckocaDHHePUE"
   },
   "source": [
    "# Part 4 - Evaluate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JwoopOWDePUH"
   },
   "outputs": [],
   "source": [
    "# Reload the model\n",
    "model_loaded = keras_model(img_width=img_width, img_height=img_height)\n",
    "model_loaded.load_weights(topDir+\"/model/model-weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "R32UCqaDePUe"
   },
   "outputs": [],
   "source": [
    "# Use model to predict test labels\n",
    "Y_hat_test = model_loaded.predict(X_test, verbose=1)\n",
    "print(Y_hat_test.shape, Y_hat_test.dtype)\n",
    "Y_hat_train = model_loaded.predict(X_train, verbose=1)\n",
    "print(Y_hat_train.shape, Y_hat_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8RFkbUXePUj"
   },
   "source": [
    "## Visualize predictions on the cross-validation test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UrI-GJVWePUl",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    try:\n",
    "        id = random.randrange(0,len(Y_hat_test))\n",
    "        print(id, X_test[id].shape)\n",
    "        fig, axarr = plt.subplots(nrows=1, ncols=3, squeeze=False, figsize=(20,20))\n",
    "        axarr[0, 0].imshow(X_test[id])\n",
    "        axarr[0, 1].imshow(Y_hat_test[id][:,:,0])\n",
    "        axarr[0, 2].imshow(Y_test[id][:,:,0])\n",
    "        plt.show()\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnWgnlwMePUp"
   },
   "source": [
    "## Visualize predictions on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nZG-Ivv1ePUr"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    try:\n",
    "        id = random.randrange(0,len(Y_hat_train))\n",
    "        print(id, X_train[id].shape)\n",
    "        fig, axarr = plt.subplots(nrows=1, ncols=3, squeeze=False, figsize=(20,20))\n",
    "        axarr[0, 0].imshow(X_train[id], aspect='equal')\n",
    "        axarr[0, 1].imshow(Y_hat_train[id][:,:,0], aspect='equal')\n",
    "        axarr[0, 2].imshow(Y_train[id][:,:,0], aspect='equal')\n",
    "        plt.show()\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4aykMBfFePUx"
   },
   "source": [
    "## Visualize predictions on the new data!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qI8gMnIvePUz"
   },
   "outputs": [],
   "source": [
    "# Load raster data into numpy array\n",
    "ds = gdal.Open('wellington-03m-rural-aerial-photos-2012-2013.tif') #get from https://data.linz.govt.nz/layer/51870-wellington-03m-rural-aerial-photos-2012-2013/   \n",
    "ary = np.dstack([ds.GetRasterBand(i).ReadAsArray() for i in range(1,4)])\n",
    "W_test = ary_to_tiles(ary, shape=(img_height, img_width))\n",
    "W_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "P7UFh1IRePU8"
   },
   "outputs": [],
   "source": [
    "W_hat_test = model_loaded.predict(W_test, verbose=1)\n",
    "print(W_hat_test.shape, W_hat_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "v6i5KZU5ePU-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    try:\n",
    "        id = random.randrange(0,len(W_test))\n",
    "        print(id, W_test[id].shape)\n",
    "        fig, axarr = plt.subplots(nrows=1, ncols=3, squeeze=False, figsize=(20,20))\n",
    "        axarr[0, 0].imshow(W_test[id])\n",
    "        axarr[0, 1].imshow(W_hat_test[id][:,:,0])\n",
    "        axarr[0, 2].imshow(W_test[id])\n",
    "        plt.show()\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HqIg7vtdePVE"
   },
   "source": [
    "# Part 5 - Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xNc4Fe7tePVG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "nz_convnet.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
